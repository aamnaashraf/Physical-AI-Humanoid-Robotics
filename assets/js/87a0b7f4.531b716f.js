"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook=globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[888],{28:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>d,default:()=>m,frontMatter:()=>l,metadata:()=>s,toc:()=>u});const s=JSON.parse('{"id":"module-4-vla-humanoids/index","title":"Module 4: VLA & Humanoid Robotics","description":"Integrate Vision-Language-Action models with humanoid robots for natural human-robot interaction and manipulation","source":"@site/docs/module-4-vla-humanoids/index.mdx","sourceDirName":"module-4-vla-humanoids","slug":"/module-4-vla-humanoids/","permalink":"/Physical-AI-Humanoid-Robotics/docs/module-4-vla-humanoids/","draft":false,"unlisted":false,"editUrl":"https://github.com/aamnaashraf/Physical-AI-Humanoid-Robotics/edit/main/docs/module-4-vla-humanoids/index.mdx","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Module 4: VLA & Humanoid Robotics","description":"Integrate Vision-Language-Action models with humanoid robots for natural human-robot interaction and manipulation","keywords":["VLA","vision language action","humanoid robotics","kinematics","manipulation","conversational AI","embodied AI"],"sidebar_position":1,"sidebar_label":"Module 4: VLA & Humanoids","estimated_time":9,"week":11,"module":4,"prerequisites":["module-3-isaac"],"learning_objectives":["Calculate forward and inverse kinematics for humanoid robots with 30+ degrees of freedom","Implement manipulation primitives (reach, grasp, place) for pick-and-place tasks","Integrate conversational AI (speech-to-text, LLMs) with robot action planning","Deploy end-to-end VLA systems that translate voice commands into physical actions"]},"sidebar":"tutorialSidebar","previous":{"title":"Reinforcement Learning & Simulation","permalink":"/Physical-AI-Humanoid-Robotics/docs/module-3-isaac/chapter-4-rl-and-simulations"},"next":{"title":"VLA \u2014 Intro & Concepts","permalink":"/Physical-AI-Humanoid-Robotics/docs/module-4-vla-humanoids/chapter-1-vla-intro"}}');var o=i(4848),r=i(8453),t=i(7874),a=i(691);const l={title:"Module 4: VLA & Humanoid Robotics",description:"Integrate Vision-Language-Action models with humanoid robots for natural human-robot interaction and manipulation",keywords:["VLA","vision language action","humanoid robotics","kinematics","manipulation","conversational AI","embodied AI"],sidebar_position:1,sidebar_label:"Module 4: VLA & Humanoids",estimated_time:9,week:11,module:4,prerequisites:["module-3-isaac"],learning_objectives:["Calculate forward and inverse kinematics for humanoid robots with 30+ degrees of freedom","Implement manipulation primitives (reach, grasp, place) for pick-and-place tasks","Integrate conversational AI (speech-to-text, LLMs) with robot action planning","Deploy end-to-end VLA systems that translate voice commands into physical actions"]},d="Module 4: VLA & Humanoid Robotics",c={},u=[{value:"Module Overview",id:"module-overview",level:2},{value:"What are VLA Models?",id:"what-are-vla-models",level:2},{value:"Module Structure",id:"module-structure",level:2},{value:"Week 11: Humanoid Kinematics &amp; Control",id:"week-11-humanoid-kinematics--control",level:3},{value:"Week 12: Manipulation Primitives",id:"week-12-manipulation-primitives",level:3},{value:"Week 13: Conversational VLA Integration",id:"week-13-conversational-vla-integration",level:3},{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Capstone Integration",id:"capstone-integration",level:2},{value:"Time Commitment",id:"time-commitment",level:2},{value:"Assessment",id:"assessment",level:2},{value:"VLA Model Architectures",id:"vla-model-architectures",level:2},{value:"Humanoid Platforms",id:"humanoid-platforms",level:2},{value:"Next Steps",id:"next-steps",level:2}];function h(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"module-4-vla--humanoid-robotics",children:"Module 4: VLA & Humanoid Robotics"})}),"\n",(0,o.jsx)(t.A,{objectives:l.learning_objectives}),"\n",(0,o.jsx)(a.A,{prereqs:l.prerequisites,estimatedTime:l.estimated_time}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Duration"}),": Weeks 11-13 | ",(0,o.jsx)(n.strong,{children:"Estimated Time"}),": 9 hours\r\n",(0,o.jsx)(n.strong,{children:"Prerequisites"}),": ",(0,o.jsx)(n.a,{href:"../module-3-isaac",children:"Module 3: Isaac Sim"})]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"module-overview",children:"Module Overview"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Vision-Language-Action (VLA) models"})," represent the cutting edge of embodied AI - systems that can:"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"See"}),": Process visual information from cameras"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Understand"}),": Interpret natural language commands"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Act"}),": Execute physical actions in the real world"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"This module brings together everything you've learned:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"ROS 2"})," (Module 1): Communication between VLA components"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Simulation"})," (Module 2): Test humanoid behaviors safely"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Perception & Navigation"})," (Module 3): Locate objects and plan paths"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Humanoid Control"})," (Module 4): Execute manipulation tasks"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"what-are-vla-models",children:"What are VLA Models?"}),"\n",(0,o.jsx)(n.p,{children:"Traditional robots require explicit programming for each task. VLA models learn from demonstration and can generalize to new scenarios:"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Traditional Approach"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'IF user says "pick up red cup" THEN:\r\n  1. Detect red cup (hard-coded vision)\r\n  2. Calculate grasp pose (hard-coded IK)\r\n  3. Execute trajectory (pre-programmed)\n'})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"VLA Approach"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'User: "Can you hand me the red cup?"\r\nVLA Model: [observes scene] \u2192 [plans actions] \u2192 [executes manipulation]\r\n  - Understands "hand me" implies grasp + bring\r\n  - Recognizes "red cup" from visual observation\r\n  - Generates manipulation trajectory end-to-end\n'})}),"\n",(0,o.jsx)(n.h2,{id:"module-structure",children:"Module Structure"}),"\n",(0,o.jsx)(n.h3,{id:"week-11-humanoid-kinematics--control",children:"Week 11: Humanoid Kinematics & Control"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Forward kinematics for multi-DOF humanoid robots"}),"\n",(0,o.jsx)(n.li,{children:"Inverse kinematics using analytical and numerical methods"}),"\n",(0,o.jsx)(n.li,{children:"Jacobian-based velocity control for smooth motions"}),"\n",(0,o.jsx)(n.li,{children:"Whole-body control for maintaining balance during manipulation"}),"\n",(0,o.jsx)(n.li,{children:"ROS 2 control interfaces (ros2_control, MoveIt 2)"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"week-12-manipulation-primitives",children:"Week 12: Manipulation Primitives"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Grasp planning and force closure analysis"}),"\n",(0,o.jsx)(n.li,{children:"Pick-and-place pipeline (approach \u2192 grasp \u2192 lift \u2192 transport \u2192 place)"}),"\n",(0,o.jsx)(n.li,{children:"Collision avoidance during manipulation"}),"\n",(0,o.jsx)(n.li,{children:"Reactive behaviors (compliant control, force feedback)"}),"\n",(0,o.jsx)(n.li,{children:"Integrating manipulation with navigation (mobile manipulation)"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"week-13-conversational-vla-integration",children:"Week 13: Conversational VLA Integration"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Speech-to-text for voice command recognition"}),"\n",(0,o.jsx)(n.li,{children:"Large Language Models (LLMs) for intent understanding"}),"\n",(0,o.jsx)(n.li,{children:"Grounding language in robot actions (task planning)"}),"\n",(0,o.jsx)(n.li,{children:"Vision-language models for object recognition"}),"\n",(0,o.jsx)(n.li,{children:"End-to-end VLA deployment (voice \u2192 plan \u2192 act)"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,o.jsx)(n.p,{children:"By the end of this module, you will be able to:"}),"\n",(0,o.jsxs)(n.p,{children:["\u2705 ",(0,o.jsx)(n.strong,{children:"Solve humanoid kinematics"}),": Compute joint angles for desired end-effector poses\r\n\u2705 ",(0,o.jsx)(n.strong,{children:"Implement manipulation"}),": Build robust pick-and-place systems\r\n\u2705 ",(0,o.jsx)(n.strong,{children:"Integrate conversational AI"}),": Connect voice commands to robot actions\r\n\u2705 ",(0,o.jsx)(n.strong,{children:"Deploy VLA systems"}),": Create end-to-end embodied AI applications\r\n\u2705 ",(0,o.jsx)(n.strong,{children:"Understand trade-offs"}),": Compare learned VLA models vs. classical planning"]}),"\n",(0,o.jsx)(n.h2,{id:"capstone-integration",children:"Capstone Integration"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"This module IS your capstone!"})," Everything culminates in Week 13:"]}),"\n",(0,o.jsxs)(n.p,{children:["Your ",(0,o.jsx)(n.strong,{children:"Autonomous Humanoid System"})," will execute this pipeline:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'Voice Input: "Please bring me the blue bottle from the table"\r\n    \u2193\r\n[Speech-to-Text] \u2192 Transcription\r\n    \u2193\r\n[LLM Task Planner] \u2192 Subtasks: navigate \u2192 locate \u2192 grasp \u2192 bring\r\n    \u2193\r\n[Navigation] (Module 3) \u2192 Move to table\r\n    \u2193\r\n[Perception] (Module 3) \u2192 Detect blue bottle via VSLAM\r\n    \u2193\r\n[Manipulation] (Module 4) \u2192 IK + grasp planning \u2192 pick up bottle\r\n    \u2193\r\n[Navigation] (Module 3) \u2192 Return to user\r\n    \u2193\r\n[Manipulation] (Module 4) \u2192 Hand over bottle\n'})}),"\n",(0,o.jsx)(n.p,{children:"This integration demonstrates how all course modules work together to build an autonomous humanoid system. Capstone project guide coming soon."}),"\n",(0,o.jsx)(n.h2,{id:"time-commitment",children:"Time Commitment"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Lectures & Reading"}),": 2 hours/week"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Hands-On Exercises"}),": 3 hours/week"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Capstone Project"}),": 25 hours (Week 13)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Total"}),": ~40 hours across 3 weeks"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"assessment",children:"Assessment"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Capstone Project"})," (Week 13):\r\nBuild the complete autonomous humanoid system with voice-driven manipulation. This is the culminating assessment that demonstrates all course learning outcomes. Detailed rubric coming soon."]}),"\n",(0,o.jsx)(n.h2,{id:"vla-model-architectures",children:"VLA Model Architectures"}),"\n",(0,o.jsx)(n.p,{children:"Current state-of-the-art VLA models include:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"RT-2"})," (Google): Vision-language-action model trained on web data + robotics"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"PaLM-E"})," (Google): Multimodal LLM grounded in embodied sensor data"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Octo"})," (UC Berkeley): Open-source generalist robot policy"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Mobile ALOHA"})," (Stanford): Bimanual manipulation with VLA integration"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"You'll learn to:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Use pre-trained VLA models (RT-2, Octo) for zero-shot generalization"}),"\n",(0,o.jsx)(n.li,{children:"Fine-tune models on custom manipulation tasks"}),"\n",(0,o.jsx)(n.li,{children:"Integrate VLA outputs with classical planners (hybrid approach)"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"humanoid-platforms",children:"Humanoid Platforms"}),"\n",(0,o.jsxs)(n.p,{children:["This course is ",(0,o.jsx)(n.strong,{children:"hardware-agnostic"}),", but examples use common platforms:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Simulation"}),": Humanoid models in Isaac Sim (no physical hardware required)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Physical Options"})," (optional):","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Unitree H1"}),": 35 DOF research humanoid"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"TIAGo"}),": Mobile manipulator (if full humanoid unavailable)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Custom Humanoid"}),": Any ROS 2-compatible platform"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Complete Module 3"}),": Ensure you understand VSLAM and Nav2"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Review Linear Algebra"}),": Brush up on rotation matrices, homogeneous transforms"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Install MoveIt 2"}),": Follow ",(0,o.jsx)(n.a,{href:"../setup/workstation",children:"Workstation Setup"})]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Start Week 11"}),": Humanoid Kinematics ",(0,o.jsx)(n.em,{children:"(Coming Soon)"})]}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Questions?"})," Check the ",(0,o.jsx)(n.a,{href:"../references/glossary",children:"Glossary"})," for VLA and humanoid terminology or consult course forums."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Previous Module"}),": ",(0,o.jsx)(n.a,{href:"../module-3-isaac",children:"Module 3: Isaac Sim"})]})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(h,{...e})}):h(e)}},691:(e,n,i)=>{i.d(n,{A:()=>o});i(6540);var s=i(4848);function o({prereqs:e,estimatedTime:n}){return Array.isArray(e)?"number"!=typeof n||n<5?(console.warn("Prerequisites: estimatedTime must be a number >= 5"),null):(0,s.jsxs)("div",{className:"prerequisites",children:[(0,s.jsx)("h3",{children:"Before You Begin"}),(0,s.jsxs)("div",{className:"estimated-time",children:["\u23f1\ufe0f Estimated Time: ",n," minutes"]}),e.length>0?(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)("p",{children:[(0,s.jsx)("strong",{children:"Prerequisites:"})," You should be familiar with the following topics:"]}),(0,s.jsx)("ul",{children:e.map((e,n)=>{const i=e.startsWith("/")?e:`/${e.replace(/^docs\//,"")}`;return(0,s.jsx)("li",{children:(0,s.jsx)("a",{href:i,children:(o=e,o.replace(/^docs\//,"").split("/").map(e=>e.split("-").map(e=>e.charAt(0).toUpperCase()+e.slice(1)).join(" ")).join(": "))})},n);var o})})]}):(0,s.jsxs)("p",{children:[(0,s.jsx)("strong",{children:"Prerequisites:"})," None - this chapter is suitable for beginners."]})]}):(console.warn("Prerequisites: prereqs must be an array"),null)}},7874:(e,n,i)=>{i.d(n,{A:()=>o});i(6540);var s=i(4848);function o({objectives:e}){return e&&Array.isArray(e)&&0!==e.length?(0,s.jsxs)("div",{className:"learning-objectives",children:[(0,s.jsx)("h3",{children:"Learning Objectives"}),(0,s.jsx)("ul",{children:e.map((e,n)=>(0,s.jsx)("li",{children:e},n))})]}):(console.warn("LearningObjectives: No objectives provided"),null)}},8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>a});var s=i(6540);const o={},r=s.createContext(o);function t(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:t(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);